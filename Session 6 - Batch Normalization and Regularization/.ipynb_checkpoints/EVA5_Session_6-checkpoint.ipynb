{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZryMFKwj5oq0"
   },
   "source": [
    "# Session 6:\n",
    "## Target: \n",
    "Run below versions for 25 epochs and report findings:<br>\n",
    "with L1 + BN <br>\n",
    "with L2 + BN <br>\n",
    "with L1 and L2 with BN <br>\n",
    "with GBN <br>\n",
    "with L1 and L2 with GBN <br>\n",
    "\n",
    "## Results:\n",
    "Parameters: 8786 <br>\n",
    "Base Model NLL with BN (Batch size 128) - Train Accuracy / Test Accuracy:  98.72% (max 98.79%) / 99.51% (max 99.53%) <br>\n",
    "with L1 + BN (Batch size 128) - Train Accuracy / Test Accuracy:  98.78% (max 98.83%) / 99.42% (max 99.43%) <br>\n",
    "with L2 + BN (Batch size 128) - Train Accuracy / Test Accuracy: 98.77% (max 98.79%) / 99.36% (max 99.38%) <br>\n",
    "with L1 and L2 with BN (Batch size 128) - Train Accuracy / Test Accuracy: 98.77% (max 98.80%) / 99.42% (max 99.45%) <br>\n",
    "with GBN (Batch size 512) - Train Accuracy / Test Accuracy: 98.3 (max 98.3) / 99.23 (max 99.24) <br>\n",
    "with L1 and L2 with GBN (Batch size 512) - Train Accuracy / Test Accuracy: 98.34% (max 98.34%) / 99.18% (max 99.20%) <br>\n",
    "\n",
    "## Analysis: <br>\n",
    "- L1 Regularization reduces the gap between train and test accuracies, in this case train accuracy increased and test accuracy decreased. As expected from regularization. <br>\n",
    "\n",
    "- L2 Regularization reduces the gap between train and test accuracies, in this case train accuracy increased and test accuracy decreased. As expected from regularization. <br>\n",
    "\n",
    "- L1+L2 regularisation seems to give better results, with test accuracy also increasing while the gap between train and test reduced.\n",
    "\n",
    "- For L1+L2 with / without GBN, it is seen that Ghost Batch normalisation seems to reduce the gap between train and test accuracy, in this case increased the train accuracy but decreased the test accuracy. <br>\n",
    "However, if we look NLL + BN and NLL + GBN comparison, NLL + BN gives higher train and test accuracy. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQQkQgSFay9z"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0m2JWFliFfKT"
   },
   "outputs": [],
   "source": [
    "# importing all the Python Packages & torch Library.\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDxHd9SdWNJK"
   },
   "source": [
    "# Define Data Transformations & Dataset for Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_efQnbQuWLco"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                        transforms.RandomRotation((-10.0, 10.0), fill=(1,)),\n",
    "                        # transforms.RandomErasing(),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "\n",
    "train_data = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
    "test_data = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4b0YpskaYd4"
   },
   "source": [
    "# Dataloader Arguments & Train / Test Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ueKm5zsaWcl",
    "outputId": "1ffe5bc1-95d5-4369-b6a7-be69963f0c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available? False\n"
     ]
    }
   ],
   "source": [
    "# seed the model to obtain consistent results\n",
    "torch.manual_seed(1)\n",
    "# this is the batch size , in 1 pas no of images passed together.\n",
    "batch_size = 128\n",
    "num_splits = 2\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"CUDA Available?\", use_cuda)\n",
    "\n",
    "# kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# dataloader arguments - something you'll fetch these from cmdprmt\n",
    "# dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=4, pin_memory=True) if use_cuda else dict(shuffle=True, batch_size=int(batch_size/2))\n",
    "\n",
    "\n",
    "\n",
    "# load the training data and perform standard normalization \n",
    "# parameter for normalization is mean and std dev.\n",
    "# train dataloader\n",
    "# train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
    "\n",
    "# test dataloader\n",
    "# test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Y30afb8J9qh"
   },
   "source": [
    "# Ghost Batch Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEL7Y08lJ8tN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GhostBatchNorm(nn.BatchNorm2d):\n",
    "    \"\"\"\n",
    "    From : https://github.com/davidcpage/cifar10-fast/blob/master/bag_of_tricks.ipynb\n",
    "\n",
    "    Batch norm seems to work best with batch size of around 32. The reasons presumably have to do \n",
    "    with noise in the batch statistics and specifically a balance between a beneficial regularising effect \n",
    "    at intermediate batch sizes and an excess of noise at small batches.\n",
    "    \n",
    "    Our batches are of size 512 and we can't afford to reduce them without taking a serious hit on training times, \n",
    "    but we can apply batch norm separately to subsets of a training batch. This technique, known as 'ghost' batch \n",
    "    norm, is usually used in a distributed setting but is just as useful when using large batches on a single node. \n",
    "    It isn't supported directly in PyTorch but we can roll our own easily enough.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_splits, eps=1e-05, momentum=0.1, weight=True, bias=True):\n",
    "        super(GhostBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum)\n",
    "        self.weight.data.fill_(1.0)\n",
    "        self.bias.data.fill_(0.0)\n",
    "        self.weight.requires_grad = weight\n",
    "        self.bias.requires_grad = bias        \n",
    "        self.num_splits = num_splits\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features*self.num_splits))\n",
    "        self.register_buffer('running_var', torch.ones(num_features*self.num_splits))\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        if (self.training is True) and (mode is False):\n",
    "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
    "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
    "        return super(GhostBatchNorm, self).train(mode)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            return F.batch_norm(\n",
    "                input.view(-1, C*self.num_splits, H, W), self.running_mean, self.running_var, \n",
    "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
    "                True, self.momentum, self.eps).view(N, C, H, W) \n",
    "        else:\n",
    "            return F.batch_norm(\n",
    "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features], \n",
    "                self.weight, self.bias, False, self.momentum, self.eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3Og4dmBao2N"
   },
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_Cx9q2QFgM7"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batchnorm):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        if (batchnorm == \"GBN\"):\n",
    "          self.conv1 = nn.Sequential(\n",
    "              \n",
    "              nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(3, 3), padding=0, bias = True), # output 26X26X4 : RF- 3X3\n",
    "              nn.ReLU(),\n",
    "              GhostBatchNorm(4, num_splits, weight=False), # Ghost Batch Normalization after each convolution.\n",
    "              nn.Dropout2d(0.05), # dropout of 5% at each layer\n",
    "              \n",
    "              nn.Conv2d(4, 8, 3), # output 24X24X8 : RF - 5x5\n",
    "              nn.ReLU(),\n",
    "              GhostBatchNorm(8, num_splits, weight=False),  \n",
    "              nn.Dropout2d(0.05),  # dropout of 5% at each layer\n",
    "\n",
    "              nn.Conv2d(8, 16, 3), # output 22X22X16 : RF - 7X7\n",
    "              nn.ReLU(),\n",
    "              GhostBatchNorm(16, num_splits, weight=False),  \n",
    "              nn.Dropout2d(0.05),  # dropout of 5% at each layer\n",
    "\n",
    "              nn.MaxPool2d(2, 2)       # output 11X11X16 : RF - 8x8 \n",
    "              )\n",
    "          self.conv2 = nn.Sequential(\n",
    "              \n",
    "            \n",
    "              nn.Conv2d(16, 16, 3), # output 9X9X16 : RF - 12x12\n",
    "              nn.ReLU(),\n",
    "              GhostBatchNorm(16, num_splits, weight=False),\n",
    "              nn.Dropout2d(0.05), # 5% dropout\n",
    "\n",
    "              nn.Conv2d(16, 16, 3, padding=1), # output 9X9X16 : RF - 16x16\n",
    "              nn.ReLU(),\n",
    "              GhostBatchNorm(16, num_splits, weight=False),\n",
    "              nn.Dropout2d(0.05), # 5% dropout\n",
    "\n",
    "              )\n",
    "          self.conv3 = nn.Sequential(\n",
    "            \n",
    "              nn.Conv2d(16, 16, 3,padding=1), # output 9X9X16 : RF - 20 X 20\n",
    "              nn.ReLU(),\n",
    "              GhostBatchNorm(16, num_splits, weight=False),\n",
    "              nn.Dropout2d(0.05), # 5% dropout\n",
    "\n",
    "              nn.Conv2d(16, 10, 1), # output 7X7X10 : RF- 20 X 20\n",
    "              nn.AvgPool2d(7) # output 1x1x10 : RF - 32x32\n",
    "\n",
    "              )\n",
    "        else: # batchnorm == \"BN\"\n",
    "          self.conv1 = nn.Sequential(\n",
    "              \n",
    "              nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(3, 3), padding=0, bias = True), # output 26X26X4 : RF- 3X3\n",
    "              nn.ReLU(),\n",
    "              nn.BatchNorm2d(4), # Batch Normalization after each convolution.\n",
    "              nn.Dropout2d(0.05), # dropout of 5% at each layer\n",
    "              \n",
    "              nn.Conv2d(4, 8, 3), # output 24X24X8 : RF - 5x5\n",
    "              nn.ReLU(),\n",
    "              nn.BatchNorm2d(8),  # Batch Normalization after each convolution.\n",
    "              nn.Dropout2d(0.05),  # dropout of 5% at each layer\n",
    "\n",
    "              nn.Conv2d(8, 16, 3), # output 22X22X16 : RF - 7X7\n",
    "              nn.ReLU(),\n",
    "              nn.BatchNorm2d(16),  # Batch Normalization after each convolution.\n",
    "              nn.Dropout2d(0.05),  # dropout of 5% at each layer\n",
    "\n",
    "              nn.MaxPool2d(2, 2)       # output 11X11X16 : RF - 8x8 \n",
    "              )\n",
    "          self.conv2 = nn.Sequential(\n",
    "              \n",
    "            \n",
    "              nn.Conv2d(16, 16, 3), # output 9X9X16 : RF - 12x12\n",
    "              nn.ReLU(),\n",
    "              nn.BatchNorm2d(16),\n",
    "              nn.Dropout2d(0.05), # 5% dropout\n",
    "\n",
    "              nn.Conv2d(16, 16, 3, padding=1), # output 9X9X16 : RF - 16x16\n",
    "              nn.ReLU(),\n",
    "              nn.BatchNorm2d(16),\n",
    "              nn.Dropout2d(0.05), # 5% dropout\n",
    "\n",
    "              )\n",
    "          self.conv3 = nn.Sequential(\n",
    "            \n",
    "              nn.Conv2d(16, 16, 3,padding=1), # output 9X9X16 : RF - 20 X 20\n",
    "              nn.ReLU(),\n",
    "              nn.BatchNorm2d(16),\n",
    "              nn.Dropout2d(0.05), # 5% dropout\n",
    "\n",
    "              nn.Conv2d(16, 10, 1), # output 7X7X10 : RF- 20 X 20\n",
    "              nn.AvgPool2d(7) # output 1x1x10 : RF - 32x32\n",
    "\n",
    "              )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x= self.conv2(x)\n",
    "        x= self.conv3(x)\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMkoC-Q_a-cv"
   },
   "source": [
    "# Print Summary of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "colab_type": "code",
    "id": "xdydjYTZFyi3",
    "outputId": "182ce884-d228-4263-d745-7a26cbfc0880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchsummary in /Users/avnishbm/Library/Python/2.7/lib/python/site-packages (1.5.1)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 26, 26]              40\n",
      "              ReLU-2            [-1, 4, 26, 26]               0\n",
      "    GhostBatchNorm-3            [-1, 4, 26, 26]               8\n",
      "         Dropout2d-4            [-1, 4, 26, 26]               0\n",
      "            Conv2d-5            [-1, 8, 24, 24]             296\n",
      "              ReLU-6            [-1, 8, 24, 24]               0\n",
      "    GhostBatchNorm-7            [-1, 8, 24, 24]              16\n",
      "         Dropout2d-8            [-1, 8, 24, 24]               0\n",
      "            Conv2d-9           [-1, 16, 22, 22]           1,168\n",
      "             ReLU-10           [-1, 16, 22, 22]               0\n",
      "   GhostBatchNorm-11           [-1, 16, 22, 22]              32\n",
      "        Dropout2d-12           [-1, 16, 22, 22]               0\n",
      "        MaxPool2d-13           [-1, 16, 11, 11]               0\n",
      "           Conv2d-14             [-1, 16, 9, 9]           2,320\n",
      "             ReLU-15             [-1, 16, 9, 9]               0\n",
      "   GhostBatchNorm-16             [-1, 16, 9, 9]              32\n",
      "        Dropout2d-17             [-1, 16, 9, 9]               0\n",
      "           Conv2d-18             [-1, 16, 9, 9]           2,320\n",
      "             ReLU-19             [-1, 16, 9, 9]               0\n",
      "   GhostBatchNorm-20             [-1, 16, 9, 9]              32\n",
      "        Dropout2d-21             [-1, 16, 9, 9]               0\n",
      "           Conv2d-22             [-1, 16, 9, 9]           2,320\n",
      "             ReLU-23             [-1, 16, 9, 9]               0\n",
      "   GhostBatchNorm-24             [-1, 16, 9, 9]              32\n",
      "        Dropout2d-25             [-1, 16, 9, 9]               0\n",
      "           Conv2d-26             [-1, 10, 9, 9]             170\n",
      "        AvgPool2d-27             [-1, 10, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 8,786\n",
      "Trainable params: 8,634\n",
      "Non-trainable params: 152\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.60\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = Net(\"GBN\").to(device)\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6q9jwl0dhtx"
   },
   "source": [
    "# Define Train and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fDefDhaFlwH"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "l1_factor = 0.00001\n",
    "\n",
    "\n",
    "  # Function to train \n",
    "'''\n",
    "  Args: \n",
    "  Model : created model to be used for training\n",
    "  device : GPU or cpu\n",
    "  train_laoded: data on which the training has to be done\n",
    "  Optimizer : the optimization algorithm to be used\n",
    "  epoch : no fo epoch \n",
    "\n",
    "'''\n",
    "def train(model, device, train_loader, optimizer, epoch, losstype):\n",
    "    model.train() # Set the model on training mode\n",
    "    pbar = tqdm(train_loader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    cross_entropy = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "      data, target = data.to(device), target.to(device) # moving the data to device\n",
    "      optimizer.zero_grad() # zero the graidents \n",
    "      output = model(data) # getting the model output\n",
    "\n",
    "      loss = 0\n",
    "      if (losstype == \"nll\") or (losstype == \"L2\"):\n",
    "        loss = F.nll_loss(output, target) # calculating the The negative log likelihood loss\n",
    "      elif (losstype == \"L1\") or (losstype == \"L1L2\"):\n",
    "        # print (\"In Train, losstype = L1 / L1L2\")\n",
    "        # Calculate loss\n",
    "        # loss = F.nll_loss(y_pred, target)\n",
    "        loss = cross_entropy(output, target)\n",
    "        # l1_crit = nn.L1Loss(reduce=False)\n",
    "        reg_loss = 0\n",
    "        for param in model.parameters():\n",
    "            reg_loss += torch.sum(abs(param))\n",
    "\n",
    "        loss += l1_factor * reg_loss\n",
    "\n",
    "      train_losses.append(loss)\n",
    "      loss.backward() # flowing the gradients backward.\n",
    "      optimizer.step() # paameter updated basd on the current gradient.\n",
    "      \n",
    "      pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "      processed += len(data)        \n",
    "      \n",
    "      pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx} Train Accuracy={100*correct/processed:0.2f}')\n",
    "      train_acc.append(100*correct/processed)\n",
    "    return train_losses, train_acc\n",
    "\n",
    "  # Function to test \n",
    "'''\n",
    "  Args: \n",
    "  Model : created model to be used for training\n",
    "  device : GPU or cpu\n",
    "  test_laoded: data on which the testing has to be done\n",
    "  \n",
    "\n",
    "'''\n",
    "def test(model, device, test_loader): #, losstype):\n",
    "  model.eval() # seting up the model for evalaution.\n",
    "  test_loss = 0 # setting the test loss to 0\n",
    "  correct = 0 # countign the no of coorect classfication.\n",
    "  with torch.no_grad(): # turn off gradients, since we are in test mode\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)   # copy the data to device.\n",
    "        output = model(data) # predict the output\n",
    "\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset) # calculating the test loss.\n",
    "  test_losses.append(test_loss)\n",
    "\n",
    "  print('\\nTest set: Average loss: {:.4f}, Test Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "  test_acc.append(100. * correct / len(test_loader.dataset))\n",
    "  return test_losses, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CI4lpKLeGsfG"
   },
   "outputs": [],
   "source": [
    "def getmisclassifiedImage(model, device, test_loader):\n",
    "  misclassified = []\n",
    "  misclassified_pred = []\n",
    "  misclassified_target = []\n",
    "  misclassfiled_list = []\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "     \n",
    "      output = model(data)\n",
    "      pred = output.argmax(dim =1, keepdim =True)\n",
    "      \n",
    "      list_misclassified = (pred.eq(target.view_as(pred)) == False)\n",
    "      batch_misclassified = data[list_misclassified]\n",
    "      batch_mis_pred = pred[list_misclassified]\n",
    "      batch_mis_target = target.view_as(pred)[list_misclassified]\n",
    "\n",
    "      misclassified.append(batch_misclassified)\n",
    "      misclassified_pred.append(batch_mis_pred)\n",
    "      misclassified_target.append(batch_mis_target)\n",
    "                                  \n",
    "  # group all the batched together\n",
    "  \n",
    "  misclassified = torch.cat(misclassified)\n",
    "  misclassified_pred = torch.cat(misclassified_pred)\n",
    "  misclassified_target = torch.cat(misclassified_target)  \n",
    "                                \n",
    " \n",
    "  misclassfiled_list.append(misclassified)\n",
    "  misclassfiled_list.append(misclassified_pred)\n",
    "  misclassfiled_list.append(misclassified_pred)\n",
    "\n",
    "  return list(map(lambda x, y, z: (x, y, z), misclassified, misclassified_pred, misclassified_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOvku-NVGqbB"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "def Plot_misclassifed(model, device, test_loader):\n",
    " plt.style.use(\"dark_background\")\n",
    " misclassified = getmisclassifiedImage(model, device, test_loader)\n",
    " num_images = 25\n",
    " fig = plt.figure(figsize=(12, 12))\n",
    " for idx, (image, pred, target) in enumerate(random.choices(misclassified, k=num_images)):\n",
    "     image, pred, target = image.cpu().numpy(), pred.cpu(), target.cpu()\n",
    "     ax = fig.add_subplot(5, 5, idx+1)\n",
    "     ax.axis('off')\n",
    "     ax.set_title('target {}\\npred {}'.format(target.item(), pred.item()), fontsize=12)\n",
    "     ax.imshow(image.squeeze())\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3tYkzvQdWbc"
   },
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MMWbLWO6FuHb",
    "outputId": "34d889dd-3781-4383-e6ad-b4ec59d4ee75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach: Loss, BN =  nll BN epoch = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.4058868885040283 batch_id=145 Train Accuracy=75.24:  16%|█▌        | 146/938 [00:15<01:21,  9.72it/s] "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pickle, os\n",
    "\n",
    "\n",
    "# with L1 + BN\n",
    "# with L2 + BN\n",
    "# with L1 and L2 with BN\n",
    "# with GBN\n",
    "# with L1 and L2 with GBN \n",
    "approach_options = [[\"nll\", \"BN\"],[\"L1\", \"BN\"], [\"L2\", \"BN\"], [\"L1L2\", \"BN\"], [\"nll\", \"GBN\"], [\"L1L2\", \"GBN\"]]\n",
    "# approach_options = [[\"nll\", \"GBN\"], [\"L1\", \"BN\"], [\"L2\", \"BN\"], [\"L1L2\", \"BN\"],  [\"L1L2\", \"GBN\"]]\n",
    "approach_dicts = {}\n",
    "gbn_misclassified = []\n",
    "gbn_predictions = []\n",
    "overwrite = True\n",
    "\n",
    "for approach in approach_options:\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "  train_acc = []\n",
    "  test_acc = []\n",
    "\n",
    "  # print (\"approach = \", approach)\n",
    "  label = approach[0] + ' with ' + approach[1]\n",
    "  if (overwrite == False) and os.path.isfile('EVAS6-' + label + '.pkl'):\n",
    "    with open ('EVAS6-' + label + '.pkl','rb') as f:\n",
    "      approach_dicts[label] = pickle.load(f)\n",
    "    #from google.colab import files\n",
    "    #files.download('EVAS6-' + label + '.pkl')\n",
    "\n",
    "  else:\n",
    "    model = Net(approach[1]).to(device) # move the model to device.\n",
    "    if (approach[0] == \"L2\") or (approach[0] == \"L1L2\"):\n",
    "      optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-5)  # intiating the SGD optimizer\n",
    "    else:\n",
    "      optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)  # intiating the SGD optimizer\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
    "    approach_dict = {}\n",
    "\n",
    "    if approach[1] == \"GBN\":\n",
    "      batch_size = 512\n",
    "      num_splits = 4   \n",
    "    else:\n",
    "      batch_size = 128\n",
    "      num_splits = 2 \n",
    "      #train_loader.batch_size = 512\n",
    "      #test_loader.batch_size = 512\n",
    "      # dataloader arguments - something you'll fetch these from cmdprmt\n",
    "    dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=4, pin_memory=True) if use_cuda else dict(shuffle=True, batch_size=int(batch_size/2))\n",
    "\n",
    "    # load the training data and perform standard normalization \n",
    "    # parameter for normalization is mean and std dev.\n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, **dataloader_args)\n",
    "\n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, **dataloader_args)\n",
    "\n",
    "    for epoch in range(1, 26):\n",
    "        print(\"Approach: Loss, BN = \",approach[0], approach[1],\"epoch =\", epoch)\n",
    "        train(model, device, train_loader, optimizer, epoch, approach[0])\n",
    "        scheduler.step()\n",
    "        test(model, device, test_loader)\n",
    "    \n",
    "    approach_dict[\"train_losses\"] = train_losses\n",
    "    approach_dict[\"train_acc\"] = train_acc\n",
    "    approach_dict[\"test_losses\"] = test_losses\n",
    "    approach_dict[\"test_acc\"] = test_acc\n",
    "    approach_dicts[label] = approach_dict\n",
    "    val_data = {'test_acc':test_acc,'test_losses':test_losses}\n",
    "    if (overwrite == True) and os.path.exists('EVAS6-' + label + '.pkl'):\n",
    "        os.remove('EVAS6-' + label + '.pkl') #this deletes the file\n",
    "    with open('EVAS6-' + label + '.pkl','wb') as f:\n",
    "      pickle.dump(val_data,f)\n",
    "  if (approach[0] == \"nll\") and (approach[1] == \"GBN\"):\n",
    "    #gbn_misclassified, _, gbn_predictions = find_missclassified(model)\n",
    "    Plot_misclassifed(model, device, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLcssDlJ-UQf"
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
    "#axs[0, 0].plot(train_losses)\n",
    "#axs[0, 0].set_title(\"Training Loss\")\n",
    "#axs[1, 0].plot(train_acc)\n",
    "#axs[1, 0].set_title(\"Training Accuracy\")\n",
    "#axs[0, 1].plot(test_losses)\n",
    "#axs[0, 1].set_title(\"Test Loss\")\n",
    "#axs[1, 1].plot(test_acc)\n",
    "#axs[1, 1].set_title(\"Test Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbdZF1KdYxjP"
   },
   "source": [
    "# Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "colab_type": "code",
    "id": "TUV1OikzYY1J",
    "outputId": "55f075e5-bd8c-4bb9-9599-b33c05328b47"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "plt.style.use(\"dark_background\")\n",
    "fig = plt.figure(figsize=(11, 9))\n",
    "\n",
    "\n",
    "for label, approach_dict in approach_dicts.items():\n",
    "  plt.plot(approach_dict['test_acc'],label = label)\n",
    "\n",
    "\n",
    "plt.title('Validation Accuracy vs. Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "if os.path.exists('evas6-acc.png'):\n",
    "    os.remove('evas6-acc.png') #this deletes the file\n",
    "fig.savefig('evas6-acc.png',dpi=150)\n",
    "plt.show()\n",
    "\n",
    "from google.colab import files\n",
    "files.download('evas6-acc.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "POT43Zr9Y3PI"
   },
   "source": [
    "# Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "colab_type": "code",
    "id": "pcjKt3SaY8vj",
    "outputId": "701019de-14ff-4bf9-a2f6-47358d8e48b2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"dark_background\")\n",
    "fig = plt.figure(figsize=(11, 9))\n",
    "from google.colab import files\n",
    "\n",
    "for label, approach_dict in approach_dicts.items():\n",
    "  plt.plot(approach_dict['test_losses'],label = label)\n",
    "  files.download('EVAS6-' + label + '.pkl')\n",
    "\n",
    "\n",
    "plt.title('Validation Loss vs. Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "fig.savefig('evas6-loss.png',dpi=150)\n",
    "files.download('evas6-loss.png')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EVA5_Session 6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
